const { Router } = require('express');
const app = Router();

let interfaces = [];
let nextId = 1;

let alert_settings = "";
let block_settings = "";

// Interface Assigned Name
const WAN = "wlan0";
const LAN = "wlan0";

const configDirectory = "/etc/suricata/";
const logDirectory = "/var/log/suricata/";
const pidDirectory = "/var/run/";
const systemdDirectory = "/lib/systemd/system/";

let queue_num = 0;

// Add the Access-Control-Allow-Origin header to allow requests from http://localhost:3000
app.use((req, res, next) => {
  res.setHeader('Access-Control-Allow-Origin', 'http://localhost:3000');
  // You can also use '*' instead of a specific origin to allow requests from any domain
  // res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS, PUT, PATCH, DELETE'); // Allow specific methods
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization'); // Allow specific headers
  next();
});

// Get network interface names
app.get('/api/interfaces-info', (req, res) => {
  const interfaces_info = os.networkInterfaces();
  const names = Object.keys(interfaces_info).filter(name => name !== 'lo');
  console.log(names);
  res.json({ interfaces: names });
});



app.get("/api/interfaces", (req, res) => {
  res.json(interfaces);
});

app.get("/api/interfaces/:id", (req, res) => {
  const found = interfaces.find((i) => i.id == req.params.id);
  found ? res.json(found) : res.status(404).json({ error: "Not found" });
});

function generateYamlContent(newEntry) {
  let configLines = [];
  configLines.push(`%YAML 1.1
---

# Suricata configuration file. In addition to the comments describing all
# options in this file, full documentation can be found at:
# https://docs.suricata.io/en/latest/configuration/suricata-yaml.html

# This configuration file generated by Suricata 7.0.8.
suricata-version: "7.0"

`);

  configLines.push(`##
## Step 1: Inform Suricata about your network
##

vars:
  # more specific is better for alert accuracy and performance
  address-groups:
    HOME_NET: "[192.168.0.0/16,10.0.0.0/8,172.16.0.0/12]"
    #HOME_NET: "[192.168.0.0/16]"
    #HOME_NET: "[10.0.0.0/8]"
    #HOME_NET: "[172.16.0.0/12]"
    #HOME_NET: "any"

    EXTERNAL_NET: "!$HOME_NET"
    #EXTERNAL_NET: "any"

    HTTP_SERVERS: "$HOME_NET"
    SMTP_SERVERS: "$HOME_NET"
    SQL_SERVERS: "$HOME_NET"
    DNS_SERVERS: "$HOME_NET"
    TELNET_SERVERS: "$HOME_NET"
    AIM_SERVERS: "$EXTERNAL_NET"
    DC_SERVERS: "$HOME_NET"
    DNP3_SERVER: "$HOME_NET"
    DNP3_CLIENT: "$HOME_NET"
    MODBUS_CLIENT: "$HOME_NET"
    MODBUS_SERVER: "$HOME_NET"
    ENIP_CLIENT: "$HOME_NET"
    ENIP_SERVER: "$HOME_NET"

  port-groups:
    HTTP_PORTS: "80"
    SHELLCODE_PORTS: "!80"
    ORACLE_PORTS: 1521
    SSH_PORTS: 22
    DNP3_PORTS: 20000
    MODBUS_PORTS: 502
    FILE_DATA_PORTS: "[$HTTP_PORTS,110,143]"
    FTP_PORTS: 21
    GENEVE_PORTS: 6081
    VXLAN_PORTS: 4789
    TEREDO_PORTS: 3544
`);

  configLines.push(`##
## Step 2: Select outputs to enable
##

# The default logging directory.  Any log or output file will be
# placed here if it's not specified with a full path name. This can be
# overridden with the -l command line parameter.
default-log-dir: ${path.join(logDirectory, newEntry.interface)}

`);

  if (newEntry.enableStatsCollection) {
    configLines.push(`# Global stats configuration
stats:
  enabled: yes
  # The interval field (in seconds) controls the interval at
  # which stats are updated in the log.
  interval: 8
  # Add decode events to stats.
  decoder-events: true
  # Decoder event prefix in stats. Has been 'decoder' before, but that leads
  # to missing events in the eve.stats records. See issue #2225.
  decoder-events-prefix: "decoder.event"
  # Add stream events as stats.
  stream-events: false
`);
  }
  configLines.push(`# Configure the type of alert (and other) logging you would like.
outputs:
`);

  if (newEntry.enableAlertsToSystemLog) {
    configLines.push(`  # a line based alerts log similar to Snort's fast.log
  - fast:
      enabled: yes
      filename: fast.log
      append: yes
      filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'
`);
  }

  if (newEntry.enableEVEJSONLog) {
    configLines.push(`  # Extensible Event Format (nicknamed EVE) event log in JSON format
  - eve-log: 
      enabled: yes
      filetype: regular #regular|syslog|unix_dgram|unix_stream|redis
      filename: eve.json
      # Enable for multi-threaded eve.json output; output files are amended with
      # an identifier, e.g., eve.9.json
      #threaded: false
      #prefix: "@cee: " # prefix to prepend to each log entry
      # the following are valid when type: syslog above
      #identity: "suricata"
      #facility: local5
      #level: Info ## possible levels: Emergency, Alert, Critical,
                   ## Error, Warning, Notice, Info, Debug
      #ethernet: no  # log ethernet header in events when available
      #redis:
      #  server: 127.0.0.1
      #  port: 6379
      #  async: true ## if redis replies are read asynchronously
      #  mode: list ## possible values: list|lpush (default), rpush, channel|publish
      #             ## lpush and rpush are using a Redis list. "list" is an alias for lpush
      #             ## publish is using a Redis channel. "channel" is an alias for publish
      #  key: suricata ## key or channel to use (default to suricata)
      # Redis pipelining set up. This will enable to only do a query every
      # 'batch-size' events. This should lower the latency induced by network
      # connection at the cost of some memory. There is no flushing implemented
      # so this setting should be reserved to high traffic Suricata deployments.
      #  pipelining:
      #    enabled: yes ## set enable to yes to enable query pipelining
      #    batch-size: 10 ## number of entries to keep in buffer

      # Include top level metadata. Default yes.
      #metadata: no

`);
  }
  configLines.push(`      types:
`);

  if (newEntry.enableAlertsToSystemLog) {
    configLines.push(`        - alert:
            # payload: yes             # enable dumping payload in Base64
            # payload-buffer-size: 4kb # max size of payload buffer to output in eve-log
            # payload-printable: yes   # enable dumping payload in printable (lossy) format
            # packet: yes              # enable dumping of packet (without stream segments)
            # metadata: no             # enable inclusion of app layer metadata with alert. Default yes
            # http-body: yes           # Requires metadata; enable dumping of HTTP body in Base64
            # http-body-printable: yes # Requires metadata; enable dumping of HTTP body in printable format

            # Enable the logging of tagged packets for rules using the
            # "tag" keyword.
            tagged-packets: yes
            # Enable logging the final action taken on a packet by the engine
            # (e.g: the alert may have action 'allowed' but the verdict be
            # 'drop' due to another alert. That's the engine's verdict)
            # verdict: yes
`);
  }

  if (newEntry.enableHTTPLog) {
    configLines.push(`  # a line based log of HTTP requests (no alerts)
  - http-log:
      enabled: yes
      filename: http.log      
`);
  }
  if (newEntry.enableAppendHTTPLog) {
    configLines.push(`      append: yes
`);
  }
  if (newEntry.enableLogExtendedHTTPinfo) {
    configLines.push(`      extended: yes     # enable this for extended logging information
`);
  }
  if (newEntry.httpLogFileType) {
    configLines.push(`      #custom: yes       # enable the custom logging format (defined by customformat)
      #customformat: "%{%D-%H:%M:%S}t.%z %{X-Forwarded-For}i %H %m %h %u %s %B %a:%p -> %A:%P"
      #filetype: ${newEntry.httpLogFileType} # 'regular', 'unix_stream' or 'unix_dgram'
`);
  }

  if (newEntry.enableTlsLog) {
    configLines.push(`  # a line based log of TLS handshake parameters (no alerts)
  - tls-log:
      enabled: yes  # Log TLS connections.
      filename: tls.log # File to store TLS logs.
      append: yes
      extended: yes     # Log extended information like fingerprint
      #custom: yes       # enabled the custom logging format (defined by customformat)
      #customformat: "%{%D-%H:%M:%S}t.%z %a:%p -> %A:%P %v %n %d %D"
      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'
      # output TLS transaction where the session is resumed using a
      # session id
      session-resumption: yes
`);
  }

  if (newEntry.enableFileStore) {
    configLines.push(`  #
  # Unlike the older filestore, metadata is not written by default
  # as each file should already have a "fileinfo" record in the
  # eve-log. If write-fileinfo is set to yes, then each file will have
  # one more associated .json files that consist of the fileinfo
  # record. A fileinfo file will be written for each occurrence of the
  # file seen using a filename suffix to ensure uniqueness.
  #
  # To prune the filestore directory see the "suricatactl filestore
  # prune" command which can delete files over a certain age.
  - file-store:
      version: 2
      enabled: no

      # Set the directory for the filestore. Relative pathnames
      # are contained within the "default-log-dir".
      dir: filestore

      # Write out a fileinfo record for each occurrence of a file.
      # Disabled by default as each occurrence is already logged
      # as a fileinfo record to the main eve-log.
      write-fileinfo: yes

      # Force storing of all files. Default: no.
      #force-filestore: yes

      # Override the global stream-depth for sessions in which we want
      # to perform file extraction. Set to 0 for unlimited; otherwise,
      # must be greater than the global stream-depth value to be used.
      #stream-depth: 0


      # Uncomment the following variable to define how many files can
      # remain open for filestore by Suricata. Default value is 0 which
      # means files get closed after each write to the file.
      max-open-files: 100

      # Force logging of checksums: available hash functions are md5,
      # sha1 and sha256. Note that SHA256 is automatically forced by
      # the use of this output module as it uses the SHA256 as the
      # file naming scheme.
      #force-hash: [sha1, md5]
`);
  }

  if (newEntry.enablePacketLog) {
    configLines.push(`  #
  # By default all packets are logged except:
  # - TCP streams beyond stream.reassembly.depth
  # - encrypted streams after the key exchange
  #
  - pcap-log:
      enabled: yes
      filename: log.pcap

      # File size limit.  Can be specified in kb, mb, gb.  Just a number
      # is parsed as bytes.
      limit: 1000mb

      # If set to a value, ring buffer mode is enabled. Will keep maximum of
      # "max-files" of size "limit"
      max-files: 2000

      # Compression algorithm for pcap files. Possible values: none, lz4.
      # Enabling compression is incompatible with the sguil mode. Note also
      # that on Windows, enabling compression will *increase* disk I/O.
      compression: none

      # Further options for lz4 compression. The compression level can be set
      # to a value between 0 and 16, where higher values result in higher
      # compression.
      #lz4-checksum: no
      #lz4-level: 0

      mode: normal # normal, multi or sguil.

      # Directory to place pcap files. If not provided the default log
      # directory will be used. Required for "sguil" mode.
      dir: /nsm_data/

      #ts-format: usec # sec or usec second format (default) is filename.sec usec is filename.sec.usec
      use-stream-depth: no #If set to "yes" packets seen after reaching stream inspection depth are ignored. "no" logs all packets
      honor-pass-rules: no # If set to "yes", flows in which a pass rule matched will stop being logged.
      # Use "all" to log all packets or use "alerts" to log only alerted packets and flows or "tag"
      # to log only flow tagged via the "tag" keyword
      #conditional: all

`);
  }

  if (newEntry.enableVerboseLog) {
    configLines.push(`# Logging configuration.  This is not about logging IDS alerts/events, but
# output about what Suricata is doing, like startup messages, errors, etc.
logging:
  # The default log level: can be overridden in an output section.
  # Note that debug level logging will only be emitted if Suricata was
  # compiled with the --enable-debug configure option.
  #
  # This value is overridden by the SC_LOG_LEVEL env var.
  default-log-level: info

  # The default output format.  Optional parameter, should default to
  # something reasonable if not provided.  Can be overridden in an
  # output section.  You can leave this out to get the default.
  #
  # This console log format value can be overridden by the SC_LOG_FORMAT env var.
  #default-log-format: "%D: %S: %M"
  #
  # For the pre-7.0 log format use:
  #default-log-format: "[%i] %t [%S] - (%f:%l) <%d> (%n) -- "

  # A regex to filter output.  Can be overridden in an output section.
  # Defaults to empty (no filter).
  #
  # This value is overridden by the SC_LOG_OP_FILTER env var.
  default-output-filter:

  # Requires libunwind to be available when Suricata is configured and built.
  # If a signal unexpectedly terminates Suricata, displays a brief diagnostic
  # message with the offending stacktrace if enabled.
  #stacktrace-on-signal: on

  # Define your logging outputs.  If none are defined, or they are all
  # disabled you will get the default: console output.
  outputs:
  - console:
      enabled: yes
      type: json
  - file:
      enabled: yes
      level: info
      filename: suricata.log
      # format: "[%i - %m] %z %d: %S: %M"
      # type: json
  - syslog:
      enabled: no
      facility: local5
      format: "[%i] <%d> -- "
      # type: json


`);
  }

  configLines.push(`##
## Netfilter integration
##

# When running in NFQ inline mode, it is possible to use a simulated
# non-terminal NFQUEUE verdict.
# This permits sending all needed packet to Suricata via this rule:
#        iptables -I FORWARD -m mark ! --mark $MARK/$MASK -j NFQUEUE
# And below, you can have your standard filtering ruleset. To activate
# this mode, you need to set mode to 'repeat'
# If you want a packet to be sent to another queue after an ACCEPT decision
# set the mode to 'route' and set next-queue value.
# On Linux >= 3.1, you can set batchcount to a value > 1 to improve performance
# by processing several packets before sending a verdict (worker runmode only).
# On Linux >= 3.6, you can set the fail-open option to yes to have the kernel
# accept the packet if Suricata is not able to keep pace.
# bypass mark and mask can be used to implement NFQ bypass. If bypass mark is
# set then the NFQ bypass is activated. Suricata will set the bypass mark/mask
# on packet of a flow that need to be bypassed. The Netfilter ruleset has to
# directly accept all packets of a flow once a packet has been marked.
nfq:
  mode: accept
#  repeat-mark: 1
#  repeat-mask: 1
  bypass-mark: 1
  bypass-mask: 1
#  route-queue: 2
#  batchcount: 20
  fail-open: yes

`);

  configLines.push(`##
## Configure Suricata to load Suricata-Update managed rules.
##

default-rule-path: /var/lib/suricata/rules

rule-files:
  - suricata.rules
  - custom.rules

##
## Auxiliary configuration files.
##

classification-file: /etc/suricata/classification.config
reference-config-file: /etc/suricata/reference.config
# threshold-file: /etc/suricata/threshold.config

##
## Include other configs
#
 
# Includes:  Files included here will be handled as if they were in-lined
# in this configuration file. Files with relative pathnames will be
# searched for in the same directory as this configuration file. You may
# use absolute pathnames too.
#include:
#  - include1.yaml
#  - include2.yaml
`);
  return configLines;
}

function generateServiceContent(pidFile, configFile, queue_num) {
  return `[Unit]
Description=Suricata IPS daemon
After=network.target network-online.target
Requires=network-online.target
Documentation=man:suricata(8) man:suricatasc(8)
Documentation=https://suricata.io/documentation/

[Service]
Type=forking
#Environment=LD_PRELOAD=/usr/lib/libtcmalloc_minimal.so.4
PIDFile=/run/${pidFile}
ExecStart=/usr/bin/suricata -D -q ${queue_num} -c ${configFile} --pidfile /run/${pidFile}
ExecReload=/usr/bin/suricatasc -c reload-rules ; /bin/kill -HUP $MAINPID
ExecStop=/usr/bin/suricatasc -c shutdown
Restart=on-failure
ProtectSystem=full
ProtectHome=true

[Install]
WantedBy=multi-user.target
`;
}

function yamlSynatxCheck(yamlContent) {
  try {
    yaml.load(yamlContent);
    console.log('✅ YAML syntax is valid.');
  } catch (err) {
    console.error('❌ YAML syntax error:');
    console.error(err.message);
  }
}

function createFolderFilesContents(newEntry) {
  try {
    if (newEntry.enableInterface === true && newEntry.interface) {
      const configPath = path.join(configDirectory, `${newEntry.interface}`);
      const logPath = path.join(logDirectory, `${newEntry.interface}`);
      let configYamlFile = "";
      // Ensure parent directories exists
      if (!fs.existsSync(configDirectory)) {
        fs.mkdirSync(configDirectory, { recursive: true });
        console.log(`Configuration Parent directory created at ${configDirectory}`);
      }

      if (!fs.existsSync(logDirectory)) {
        fs.mkdirSync(logDirectory, { recursive: true });
        console.log(`Configuration Parent directory created at ${logDirectory}`);
      }

      if (!fs.existsSync(pidDirectory)) {
        fs.mkdirSync(pidDirectory, { recursive: true });
        console.log(`Configuration Parent directory created at ${pidDirectory}`);
      }

      if (!fs.existsSync(systemdDirectory)) {
        fs.mkdirSync(systemdDirectory, { recursive: true });
        console.log(`Configuration Parent directory created at ${systemdDirectory}`);
      }

      if (fs.existsSync(configPath)) {
        console.log(`Folder "${configPath}" already exists.`);

        configYamlFile = path.join(configPath, `suricata-${newEntry.interface}.yaml`);
        if (fs.existsSync(configYamlFile)) {
          yamlSynatxCheck(generateYamlContent(newEntry));
          fs.writeFileSync(configYamlFile, generateYamlContent(newEntry), 'utf8');
          console.log(`Updated: ${configYamlFile}`);
        }

      } else {
        // Create new folder and add initial files 
        fs.mkdirSync(configPath);
        console.log(`Folder "${configPath}" created.`);

        // Create info.txt
        configYamlFile = path.join(configPath, `suricata-${newEntry.interface}.yaml`);
        yamlSynatxCheck(generateYamlContent(newEntry));
        fs.writeFileSync(configYamlFile, generateYamlContent(), 'utf8');
        console.log(`Created: ${configYamlFile}`);
      }

      if (fs.existsSync(logPath)) {
        console.log(`Folder "${logPath}" already exists.`);
      } else {
        // Create new folder
        fs.mkdirSync(logPath);
        console.log(`Folder "${logPath}" created.`);
      }


      const pidFile = `suricata-${newEntry.interface}.pid`;

      const systemdServiceFile = path.join(systemdDirectory, `suricata-${newEntry.interface}.service`);
      if (fs.existsSync(systemdServiceFile)) {
        fs.writeFileSync(systemdServiceFile, generateServiceContent(pidFile, configYamlFile, 0), 'utf8');
        console.log(`Updated: ${systemdServiceFile}`);
      }
      else {
        fs.writeFileSync(systemdServiceFile, generateServiceContent(pidFile, configYamlFile, 0), 'utf8');
        console.log(`Created: ${systemdServiceFile}`);
      }
    }
  }
  catch (err) {
    console.error('Error occurred:', err.message);
  }
}

function deleteFolderFilesContents(data) {
  const ifce = data.interface;
  try {
    // For deleting configuration specific folder and its files of this interface
    fs.rmSync(path.join(configDirectory, `${ifce}`), { recursive: true });
    console.log("Folder \"" + path.join(configDirectory, `${ifce}`) + "\" Removed.");

    // For deleting Log specific folder and its files of this interface
    fs.rmSync(path.join(logDirectory, `${ifce}`), { recursive: true });
    console.log("Folder \"" + path.join(logDirectory, `${ifce}`) + "\" Removed");

    // For deleting system service specific  file of this interface
    fs.rmSync(path.join(systemdDirectory, `suricata-${ifce}.service`));
    console.log("Removed: \"" + path.join(systemdDirectory, `suricata-${ifce}.service`) + "\"");
  }
  catch (err) {
    console.error('Error occurred:', err.message);
  }
}


app.post("/api/interfaces", (req, res) => {
  const newEntry = { ...req.body, id: nextId++ };
  createFolderFilesContents(newEntry);
  interfaces.push(newEntry);
  res.json(newEntry);
});

app.put("/api/interfaces/:id", (req, res) => {
  const index = interfaces.findIndex((i) => i.id == req.params.id);
  if (index !== -1) {
    const preInterface = interfaces[index];

    if (JSON.stringify(req.body) !== JSON.stringify(preInterface)) {
      interfaces[index] = { ...req.body, id: interfaces[index].id };
      createFolderFilesContents(interfaces[index]);
    }
    res.json(interfaces[index]);
  } else {
    res.status(404).json({ error: "Not found" });
  }
});

app.delete("/api/interfaces/:id", (req, res) => {
  const id = parseInt(req.params.id);
  const index = interfaces.findIndex((i) => i.id === id);
  if (index !== -1) {
    deleteFolderFilesContents(interfaces[index]);
    interfaces.splice(index, 1);
    res.json({ message: "Deleted successfully" });
  } else {
    res.status(404).json({ error: "Not found" });
  }
});

app.get("/api/alert_settings", (req, res) => {
  res.json(alert_settings);
});

app.post("/api/alert_settings", (req, res) => {
  const new_settings = req.body;
  alert_settings = new_settings;
  res.json(alert_settings);
});

app.get("/api/block_settings", (req, res) => {
  res.json(block_settings);
});

app.post("/api/block_settings", (req, res) => {
  const new_settings = req.body;
  block_settings = new_settings;
  res.json(block_settings);
});


function parseSuricataLogs(logLines) {
  const results = [];

  const regex = /^(\d{2}\/\d{2}\/\d{4}-\d{2}:\d{2}:\d{2}\.\d+)\s+\[\*\*\]\s+\[(\d+):(\d+):(\d+)\]\s+(.*?)\s+\[\*\*\]\s+\[Classification: (.*?)\]\s+\[Priority: \d+\]\s+\{(\w+)\}\s+([\d.]+):(\d+)\s+->\s+([\d.]+):(\d+)/;

  for (let i = 0; i < logLines.length; i++) {
    const line = logLines[i];
    const match = line.match(regex);

    if (match) {
      const [
        _,
        date,
        action,
        gid,
        sid,
        rev,
        description,
        classification,
        protocol,
        sourceIP,
        sourcePort,
        destinationIP,
        destinationPort
      ] = match;

      results.push({
        date,
        action,
        protocol,
        sourceIP,
        sourcePort,
        destinationIP,
        destinationPort,
        description: `${description} (${classification})`,
        gidSidRev: `${gid}:${sid}:${rev}`
      });
    } else {
      console.warn("No match for log line:", line);
    }
  }

  return results;
}


app.get('/api/alertlogs', (req, res) => {
  console.log("Request come");
  if (alert_settings !== "") {
    const logText = fs.readFileSync('/var/log/suricata/fast.log.1', 'utf-8');
    const lines = logText.split('\n').filter(Boolean);
    const parsedLogs = parseSuricataLogs(lines.slice(-alert_settings.numAlertsToDisplay).reverse());
    res.json(parsedLogs);
  }
});


app.get('/api/blocklogs', (req, res) => {
  console.log("Request come");
  if (block_settings !== "") {
    const logText = fs.readFileSync('/var/log/suricata/fast.log.1', 'utf-8');
    const lines = logText.split('\n').filter(Boolean);
    const parsedLogs = parseSuricataLogs(lines.slice(-block_settings.numAlertsToDisplay).reverse());
    res.json(parsedLogs);
  }
});


const absolutePath = '/var/log/suricata';

app.get('/api/files', (req, res) => {
  console.log("requested");
  fs.readdir(absolutePath, (err, files) => {
    if (err) return res.status(500).json({ error: 'Unable to read folder' });
    res.json(files);
  });
});

// Get content of a file
app.get('/api/file', (req, res) => {
  const filePath = path.join(absolutePath, req.query.path);

  if (!filePath) {
    return res.status(400).json({ error: 'Missing file path' });
  }

  const filepath = path.resolve(filePath);

  fs.readFile(filepath, 'utf8', (err, data) => {
    if (err) return res.status(500).json({ error: 'Failed to read file', detail: err.message });
    res.send(data);
  });
});



module.exports = app;